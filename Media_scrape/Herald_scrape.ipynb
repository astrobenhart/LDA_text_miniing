{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herald Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook ncludes the code needed to scrape articles from [The Herald](https://www.nzherald.co.nz/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nzherald.co.nz/business/news/headlines.cfm?c_id=3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = requests.get(url)\n",
    "print(r1.status_code)\n",
    "\n",
    "coverpage = r1.content\n",
    "\n",
    "soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "# coverpage_news = soup1.find_all('a', href=lambda href: href and 'business' in href)\n",
    "# coverpage_news = soup1.find_all('article', class_=lambda x: x != 'marketing---text-below')\n",
    "coverpage_news = soup1.find_all('article')\n",
    "len(coverpage_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:50<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "news_contents = []\n",
    "news_dates = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "for n in tqdm(np.arange(0, len(coverpage_news))):\n",
    "        \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    try:\n",
    "        article = requests.get('https://www.nzherald.co.nz'+str(link))\n",
    "    except:\n",
    "        article = 0\n",
    "    if article != 0:\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        article_date = soup_article.find_all('div', class_='publish')\n",
    "        body = soup_article.find_all('div', id='article-body')\n",
    "        if body != []:\n",
    "            x = body[0].find_all('p')\n",
    "            for child in article_date[0].find_all(\"div\"):\n",
    "                child.decompose()\n",
    "            article_date = article_date[0].get_text()\n",
    "            # Unifying the paragraphs\n",
    "            list_paragraphs = []\n",
    "            try:\n",
    "                for p in np.arange(0, len(x)):\n",
    "                    paragraph = x[p].get_text()\n",
    "                    list_paragraphs.append(paragraph)\n",
    "                    final_article = \" \".join(list_paragraphs)\n",
    "            except:\n",
    "#                 print('no info')\n",
    "                pass\n",
    "\n",
    "    else:\n",
    "#         print('not an article')\n",
    "        final_article = np.nan\n",
    "        article_date = np.nan\n",
    "    news_contents.append(final_article)\n",
    "    news_dates.append(article_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame({'Article Content': news_contents, 'Article Link': list_links, 'Article Date': news_dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.dropna(axis=0)\n",
    "df_features = df_features.drop_duplicates(subset ='Article Content', keep='first')\n",
    "df_features = df_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "now=pd.datetime.now()\n",
    "df_features.to_pickle('herald_business_{}_{}-{}.pkl'.format(now.day, now.month, now.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 11, 23, 19, 12, 51, 286754)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
