{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Herald Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will run through the same process as in lda_tutorial.ipynb using the Herald data that we scraped earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "from sklearn.utils import shuffle\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('QT5Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/herald_business_tutorial.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in df['Article Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word frequency filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_flat = [val for sublist in doc_clean for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_flat_df = pd.DataFrame({'words': doc_clean_flat})\n",
    "doc_clean_flat_df = doc_clean_flat_df.words.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_flat_df = pd.DataFrame({'word': doc_clean_flat_df.index, 'count':doc_clean_flat_df}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_flat_df['count_norm'] = doc_clean_flat_df['count']/doc_clean_flat_df['count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_flat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.1\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.bar(x=doc_clean_flat_df[doc_clean_flat_df.count_norm>cutoff]['word'], height=doc_clean_flat_df[doc_clean_flat_df.count_norm>cutoff]['count'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_filter = doc_clean_flat_df[doc_clean_flat_df.count_norm>cutoff]['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_list = list()\n",
    "for item in doc_clean_flat:\n",
    "    tokenized = nltk.word_tokenize(item)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "    chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "\n",
    "    chunked = chunkParser.parse(tagged)\n",
    "    POS_list.append(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_list = [i[0] for i in POS_list]\n",
    "POS_cat = [i[1] for i in POS_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.bar(x=Counter(POS_cat).keys(), height=Counter(POS_cat).values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_POS_list = []\n",
    "for word in POS_list:\n",
    "    if word[1] not in ['IN', 'MD', 'CD']:\n",
    "        new_POS_list.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_filter = list(dict.fromkeys(new_POS_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment',\n",
       " 'recently',\n",
       " 'involved',\n",
       " 'board',\n",
       " 'appointment',\n",
       " 'panel',\n",
       " 'voluntary',\n",
       " 'organisation',\n",
       " 'lead',\n",
       " 'interview']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_filter = set(word_filter)\n",
    "doc_clean_2 = []\n",
    "for doc in doc_clean:\n",
    "    doc_2 = [x for x in doc if (x in word_filter) and (x in POS_filter)]\n",
    "    doc_clean_2.append(doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean_2)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean_2]\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_logger = PerplexityMetric(corpus=doc_term_matrix, logger='shell')\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=500, chunksize=5, update_every=0, eta='auto', iterations=5, random_state=12345, callbacks=[perplexity_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.145*\"say\" + 0.092*\"business\" + 0.040*\"people\" + 0.037*\"firm\" + 0.030*\"big\" + 0.027*\"many\" + 0.026*\"year\" + 0.026*\"–\"'), (1, '0.263*\"said\" + 0.039*\"office\" + 0.039*\"week\" + 0.037*\"•\" + 0.037*\"service\" + 0.032*\"cost\" + 0.031*\"also\" + 0.030*\"problem\"'), (2, '0.105*\"investment\" + 0.072*\"investor\" + 0.067*\"bank\" + 0.066*\"fund\" + 0.055*\"financial\" + 0.045*\"interest\" + 0.043*\"asset\" + 0.041*\"rate\"'), (3, '0.109*\"company\" + 0.080*\"price\" + 0.067*\"share\" + 0.063*\"saudi\" + 0.061*\"port\" + 0.061*\"cent\" + 0.051*\"year\" + 0.039*\"u\"'), (4, '0.058*\"change\" + 0.057*\"sustainable\" + 0.055*\"risk\" + 0.051*\"say\" + 0.049*\"—\" + 0.046*\"need\" + 0.043*\"system\" + 0.042*\"climate\"'), (5, '0.122*\"auckland\" + 0.102*\"property\" + 0.053*\"new\" + 0.053*\"business\" + 0.053*\"centre\" + 0.041*\"building\" + 0.038*\"development\" + 0.036*\"site\"'), (6, '0.182*\"cent\" + 0.056*\"milk\" + 0.054*\"market\" + 0.052*\"china\" + 0.044*\"food\" + 0.042*\"year\" + 0.038*\"•\" + 0.037*\"company\"'), (7, '0.216*\"new\" + 0.167*\"zealand\" + 0.044*\"technology\" + 0.043*\"company\" + 0.036*\"sector\" + 0.031*\"energy\" + 0.028*\"also\" + 0.026*\"country\"'), (8, '0.083*\"—\" + 0.049*\"people\" + 0.039*\"think\" + 0.037*\"account\" + 0.034*\"time\" + 0.028*\"customer\" + 0.028*\"pay\" + 0.028*\"get\"'), (9, '0.077*\"company\" + 0.074*\"work\" + 0.068*\"said\" + 0.067*\"staff\" + 0.053*\"employee\" + 0.048*\"also\" + 0.044*\"working\" + 0.042*\"job\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
